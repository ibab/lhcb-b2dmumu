
\chapter{Outline of the analysis}

The goal of the analysis is to determine a limit on the branching fraction of the decay $\PBzero\to\APDzero\APmuon\Pmuon$, using events where the $\APDzero$ decays to $\PKplus\Ppiminus$.
This represents the first measurement searching for this decay channel so far.

To minimize experimenter's bias, a blind analysis is performed.
This is achieved by removing a part of the reconstructed mass spectrum of the decay channel that fully contains the signal decay.
The analysis is developed without access to this signal region, which is unblinded (taken into account) only once the analysis is finalized.
A useful result of the analysis before unblinding is the calculation of an \emph{expected limit}, which represents an estimate of the limit that will be achieved after unblinding, under the assumption that there are no signal events in the signal region.

The first step of the analysis is a loose preselection involving cuts on kinematic properties and measures of fit quality for the decay.
Following this, several veto cuts are applied to reject physical backgrounds.
In order to reduce the amount of combinatorial background, a multivariate classifier (Gradiend Boosted Decision Trees) is employed.
Various signal efficiencies resulting from the trigger, the reconstruction, the preselection, vetos on physical backgrounds and the multivariate classification need to be taken into account.
These are determined using a simulated signal data sample.

To reduce the systematic uncertainty of the branching fraction measurement, it is useful to perform a simultaneous measurement of the branching fraction of a \emph{normalization channel} and to normalize the signal branching fraction to this measurement.
The decay $\PBzero\to\PJpsi\PKstar$ is an ideal candidate for this, because it features both a large branching fraction, which reduces the uncertainty in the normalization process, and a final state ($\PKplus\Ppiminus\APmuon\Pmuon$) that is identical to the one of the signal decay.

When calculating the signal branching fraction $BR_\text{sig}$, the control channel is taken into account by
\begin{equation}
  BR_\text{sig} = \frac{BR_\text{norm} \varepsilon_\text{norm}}{N_\text{norm} \varepsilon_\text{sig}} N_\text{sig}\:,
\end{equation}
where $BR_\text{norm}$ refers to a known branching fraction of the signal decay (e.g. from previous measurements), $\varepsilon_\text{sig}$ and $\varepsilon_\text{norm}$ refer to the total efficiencies of the signal and normalization channels, respectively, and $N_\text{sig}$ and $N_\text{norm}$ refer to the measured yields of the signal and normalization channel.

The factor
\begin{equation}
  \alpha = \frac{BR_\text{norm} \varepsilon_\text{norm}}{N_\text{norm} \varepsilon_\text{sig}}
  \label{eq:alpha}
\end{equation}
is referred to as the \emph{normalization constant}.
It translates the measured number of signal decays to the measured branching fraction.

Statistical models are developed for the reconstructed mass of both the signal and and normalization channels.
These can be used to determine the number of signal or normalization events remaining after the selection procedure through a Maximum Likelihood Estimate (MLE).
By incorporating \eqref{eq:alpha} into the fit, the signal branching fraction can be determined directly as a parameter of the model.

Based on this model, the Profile Likelihood Ratio method is used to calculate limits on the branching fraction, taking into account nuisance parameters.
In order to calculate an expected limit for the signal decay branching fraction, toy simulations of the blinded signal region are generated using a model of the background distribution fit to the sidebands of the mass distribution.
Each of the toys is combined with the data outside the blinded region and a limit is calculated.
The distribution of calculated limits is used to determine an expected limit on the signal decay branching fraction.

\chapter{Selection} % 18

\section{Dataset}

\begin{itemize}
  \item 3 inverse femtobarn from LHCb Run I (2011+2012)
  \item Explain trigger lines used
  \item I used phase space simulation of $B^0\to\overline{D^0}μ^+μ^-$
  \item Which trigger lines do we use for the control channel? (Same, lol)
\end{itemize}

\section{Preselection criteria}

\begin{itemize}
  \item This explains the stripping line cuts
  \item What did we use for the control channel? Same (lol) with some differences: explain
  \item How is the blinding cut applied? Use objective criteria!
  \item How many events end up in the blinded sample?
  \item How many events do we have in our control channel?
\end{itemize}

\section{Vetoes to reject specific physical backgrounds}

\begin{figure}
  \centering
  \missingfigure[figwidth=\textwidth]{}
  \caption{A figure of the $B\to Dμμ$ dataset after stripping, with blinding}
\end{figure}

\begin{figure}
  \centering
  \missingfigure[figwidth=\textwidth]{}
  \caption{Show that phase space ends before the Psi(2S)}
\end{figure}

\begin{figure}
  \centering
  \missingfigure[figwidth=\textwidth]{}
  \caption{Figure for $B^0\to D^{*-}μ^+ν$ background veto}
\end{figure}

\begin{itemize}
  \item How does the dataset look after stripping? Dem $J/ψ$ peaks lol
  \item Remove backgrounds with $J/ψ$ through cut on $μμ$ inv mass
  \item Reconstruct $D^*$ mass through $\overline{D}^0μ^-$, plot of the peak, veto $B^0\to D^{*-}μ^+ν$
  \item Treat other part reco as well???
\end{itemize}

\section{Multivariate classification}

\begin{itemize}
  \item What is all this ML anyway?
  \item Way too much background: Looking for N signal events in dataset of K
  \item Use ML methods to classify combine many variables into one discriminant
  \item Choose optimal cut on discriminant according to a criterium
  \item Several libraries tested
    \begin{itemize}
      \item TMVA
      \item scikit-learn
      \item XGBoost
    \end{itemize}
  \item Best performance speed-wise and in AUC: XGBoost $\rightarrow$ choose this
\end{itemize}

\subsection{Gradient tree boosting}

\begin{itemize}
  \item Short explanation of how the algorithm works
  \item Explain that this is basically steepest descent 
  \item Nice: invariant under scaling of inputs
  \item Check this out: \url{http://homes.cs.washington.edu/~tqchen/pdf/BoostedTree.pdf}
\end{itemize}

\subsection{Features used in the classification}

\begin{itemize}
  \item Use phase space signal MC as signal proxy
  \item Use right sideband as bkg proxy. Why?
  \item Dem control plots
  \item Plots of signal MC, sweighted control and control MC
  \item Use resampling for PID variables
  \item correlation matrix
  \item importances?
\end{itemize}

\begin{figure}
  \centering
  \missingfigure[figwidth=\textwidth]{}
  \caption{Overview of all variables used for multivariate classification: signal and control MC, data sidebands}
\end{figure}

\subsubsection{PID resampling}

\begin{figure}
  \centering
  \missingfigure[figwidth=\textwidth]{}
  \caption{PID resampling control plots}
\end{figure}

\subsection{Classification of signal and background}

\begin{itemize}
  \item Plot of classifier output for signal MC, sidebands and control signal as well
  \item Plot of classifier output for control sweighted signal and MC
  \item Give a KS test value for the control channel that shows that the classifier is unbiased
\end{itemize}

\begin{figure}
  \centering
  \missingfigure[figwidth=\textwidth]{}
  \caption{Plot of classifier output for signal MC, sidebands and control signal}
\end{figure}

\begin{figure}
  \centering
  \missingfigure[figwidth=\textwidth]{}
  \caption{Plot of classifier output for control sweighted signal and MC}
\end{figure}

\subsection{Optimization of the Figure of Merit}

\begin{itemize}
  \item Choose an appropriate signal window for this (objectively)
  \item We want to optimize the classifier cut according to an objective criterium
  \item Use punzi figure of merit (formula)
  \item Use signal efficiency for s, don't need total number of signal candidates as it can be factorized and doesn't affect the maximum
  \item For background efficiency, take efficiency on sidebands. For total number of bkg candidates, perform initial fit (show plot)
  \item Show plot of FOM depending on cut point
  \item List maximum value, explain that this was used to cut on clf output
\end{itemize}

\begin{figure}
  \centering
  \missingfigure[figwidth=\textwidth]{}
  \caption{Plot of figure of merit depending on cut point}
\end{figure}

\section{Determination of the signal efficiency}

\begin{itemize}
  \item Get stripping efficiency from MC
  \item Get trigger efficiency from TISTOS on control channel
\end{itemize}

\chapter{Determination of expected signal branching ratio} % 15

\begin{itemize}
  \item Give an overview of how this was done
\end{itemize}

\section{Maximum Likelihood estimation}

In order to extract an estimate of the number of signal events from the dataset, a \gls{MLE} is performed.
The \gls{MLE} is a method to determine the parameters $θ$ of a probability distribution $p(x | θ)$, given the data $x$.
It is given by
\begin{equation}
  θ^* = \mathrm{argmax}\ p(θ | x)
  \label{eq:mle}
\end{equation}
where $p(θ | x)$ is the \textit{Likelihood}, which can be obtained by fixing the data $x$ in the probability distribution $p(x | θ)$ and varying the parameters $θ$.

If we are dealing with $N$ identically distributed and uncorrelated events $\vec{x}$, we can express the Likelihood $\mathcal{L}$ as
\begin{equation}
  \mathcal{L}(θ | \vec{x}) = \prod_i^N p(x_i | θ)\:.
\end{equation}

In high energy physics, statistical models often have to discriminate between different categories of events (\eg \textit{signal} or \textit{background}).
This can be realized as a \textit{Mixture Model}
\begin{equation}
  \mathcal{L}(θ, \vec{f} | \vec{x}) = \prod_i^N \sum_j^K f_j p_j(x_i | θ)\:,
\end{equation}
where $K$ is the number of categories and $f_j$ is the mixture weight of the $j$th category with
\begin{equation}
  \sum_j f_j = 1\:.
\end{equation}
The events originating from mixture category $j$ are distributed according to $p_j(x | θ)$.

From this, estimates $f_j^*$ for the mixture weights can be inferred via \eqref{eq:mle}.
But in high energy physics, we are rarely interested in the mixture weights, and more often in the total number of events in a certain category, \eg the number of signal events.
Such an estimate can be obtained through
\begin{equation}
  N_j = f_j N_\text{total}\:.
\end{equation}
To determine the uncertainty of our estimate $N_j$, the uncertainties of $f_j$ (determined from the \gls{MLE}) and of $N_\text{total}$ have to be propagated, where we have to take into account that $N_\text{total}$ is the result of a Poisson counting experiment.

This manual step can be avoided by modeling the Poissonian fluctuation directly as part of the Likelihood:
\begin{equation}
  \mathcal{L}(θ, \vec{f}, n | \vec{x}, N) = \frac{n^N\mathrm{e}^{-n}}{N!}  \prod_i^N \sum_j^K f_j p_j(x_i | θ)\:,
\end{equation}
where $n$ is the expected value of $N$.

By performing the variable transformation $n f_j \to N_j$ and neglecting the constant factor $\frac{1}{N!}$, we obtain
\begin{equation}
  \mathcal{L}(θ, N_j | \vec{x}, N) = \mathrm{e}^{-\sum_j N_j}  \prod_i^N \sum_j^K N_j p_j(x_i | θ)\:.
\end{equation}
This model, which is referred to as the \textit{Extended Likelihood}\cite{Lyons1986}, allows for a direct estimate of $N_j$.

\begin{itemize}
  \item Mention RooFit
\end{itemize}

\section{The Profile Likelihood Ratio}

\begin{itemize}
  \item We want to get a limit, need PLR
  \item Explain Wilke's theorem
  \item How do we go about calculating an expected limit? $\rightarrow$ lots of toys
\end{itemize}

\section{Signal model}

\begin{figure}
  \centering
  \missingfigure[figwidth=\textwidth]{}
  \caption{Fit on signal MC}
\end{figure}

\begin{itemize}
  \item Double Gaussian
  \item Constrain this from simulation
\end{itemize}

\section{Background models}

\begin{itemize}
  \item Exponential, one resonant in $D$, one not
\end{itemize}

\begin{figure}
  \centering
  \missingfigure[figwidth=\textwidth]{}
  \caption{Fit to sidebands}
\end{figure}

\section{Validation of fit model}

\begin{itemize}
  \item Maybe generate toys and look at some pulls
\end{itemize}

\section{Normalization channel}

\begin{figure}
  \centering
  \missingfigure[figwidth=\textwidth]{}
  \caption{Fit in normalization channel}
\end{figure}

\begin{itemize}
  \item This is $B^0\to J/ψK^{*0}$
  \item Why do we want a normalization channel? $\rightarrow$ to reduce systematic uncertainties
  \item Choose similar trigger/stripping setup
  \item Show fits and give number of expected signal events
\end{itemize}

\section{Normalization constant}

\begin{itemize}
  \item Give formula for normalization constant $α$
  \item Calculate value of $α$
\end{itemize}

\section{Calculating the expected branching ratio}

\begin{itemize}
  \item How do we do this?
  \item Put alpha as constraint into fit
  \item Make toys, calculate limit for each
  \item Take median, show plot
\end{itemize}

\chapter{Systematic uncertainties} % 3

\begin{itemize}
  \item Which systematic uncertainties do we have?
  \item PID resampling
  \item Shape of signal
  \item Trigger efficiency
  \item Dimuon mass resolution
\end{itemize}

\chapter{Conclusion and outlook} % 1

