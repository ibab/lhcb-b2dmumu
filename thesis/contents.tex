
\chapter{Introduction} % 1

\begin{itemize}
  \item What has been measured?
  \item Which dataset?
  \item Which experiment?
  \item What are the goals? 
    \begin{itemize}
      \item Investigate non-factorizable hadronic influence: A way to test theory
      \item Useful to understand effects in $B^0\to K^{*0}\mu\mu$
    \end{itemize}
  \item What's the current status of the analysis?
  \item What are the results of the measurement?
    \begin{itemize}
      \item An expected limit on the signal branching fraction
    \end{itemize}
\end{itemize}

\chapter{Theory} % 4

\section{The Standard Model of particle physics}

\begin{itemize}
  \item What is the standard model?
  \item Why are rare decays interesting?
  \item Challenges in perturbative QCD
\end{itemize}

\section[The decay \decay]{The decay \bolddecay}

\begin{itemize}
  \item summarize theory paper
\end{itemize}

\chapter{The LHCb Experiment} % 6

Some general info
\begin{itemize}
  \item What are the goals of the experiment?
  \item When was it built?
  \item How much data was taken so far?
\end{itemize}

\section{The Large Hadron Collider}

\begin{itemize}
  \item A nice figure showing the LHC and other facilities
  \item When was the LHC built?
  \item Which are the four large experiments and what do they do?
\end{itemize}

\section{Experimental setup}

Explain the most important components of the LHCb detector.
Take care to mention how these aid the final analysis.
\begin{itemize}
  \item VELO
  \item Tracking and Magnet
  \item RICH for particle identification
  \item muon detector
\end{itemize}

\section{Trigger and data processing}

\chapter{Outline of the analysis} % 2

\begin{itemize}
  \item We want to determine limit on $B^0\to \overline{D}^0μ^+μ^-$
  \item First measurement
  \item Use control channel $B^0\to J/ψ K^*$ to reduce systematic uncertainties
  \item Blinded analysis
  \item Simple preselection
  \item Vetoes to reject peaking backgrounds: $J/ψ K^*$, $D^{*-}μν$
  \item Apply multivariate classifier to get rid of background
  \item Determine trigger efficiency through TISTOS on control channel
  \item Determine selection efficiencies trough simulation
  \item Perform fit to control channel to derive control yield
  \item Perform 2-dimensional fit to $B$ and $D$ mass to derive signal yield
  \item Calculate $α$ and use it to transform to branching fraction
  \item Would be neat to write down how to calculate $α$
  \item Use Profile Likelihood Ratio to derive limit on branching fraction
\end{itemize}

\chapter{Selection} % 18

\section{Dataset}

\begin{itemize}
  \item 3 inverse femtobarn from LHCb Run I (2011+2012)
  \item Explain trigger lines used
  \item I used phase space simulation of $B^0\to\overline{D^0}μ^+μ^-$
  \item Which trigger lines do we use for the control channel? (Same, lol)
\end{itemize}

\section{Preselection criteria}

\begin{itemize}
  \item This explains the stripping line cuts
  \item What did we use for the control channel? Same (lol) with some differences: explain
  \item How is the blinding cut applied? Use objective criteria!
  \item How many events end up in the blinded sample?
  \item How many events do we have in our control channel?
\end{itemize}

\section{Vetoes to reject specific physical backgrounds}

\begin{figure}
  \centering
  \missingfigure[figwidth=0.7\textwidth]{The $B\to Dμμ$ dataset after stripping, with blinding}
  \caption{A figure of the $B\to Dμμ$ dataset after stripping, with blinding}
\end{figure}

\begin{figure}
  \centering
  \missingfigure[figwidth=0.7\textwidth]{Reconstructing the $D^*$ invariant mass and spotting background}
  \caption{Figure for $B^0\to D^{*-}μ^+ν$ background veto}
\end{figure}

\begin{itemize}
  \item How does the dataset look after stripping? Dem $J/ψ$ peaks lol
  \item Remove backgrounds with $J/ψ$ through cut on $μμ$ inv mass
  \item Reconstruct $D^*$ mass through $\overline{D}^0μ^-$, plot of the peak, veto $B^0\to D^{*-}μ^+ν$
  \item Treat other part reco as well???
\end{itemize}

\section{Multivariate classification}

\begin{itemize}
  \item What is all this ML anyway?
  \item Way too much background: Looking for N signal events in dataset of K
  \item Use ML methods to classify combine many variables into one discriminant
  \item Choose optimal cut on discriminant according to a criterium
  \item Several libraries tested
    \begin{itemize}
      \item TMVA
      \item scikit-learn
      \item XGBoost
    \end{itemize}
  \item Best performance speed-wise and in AUC: XGBoost $\rightarrow$ choose this
\end{itemize}

\subsection{Gradient tree boosting}

\begin{itemize}
  \item Short explanation of how the algorithm works
  \item Explain that this is basically steepest descent 
  \item Nice: invariant under scaling of inputs
  \item Check this out: \url{http://homes.cs.washington.edu/~tqchen/pdf/BoostedTree.pdf}
\end{itemize}

\subsection{Features used in the classification}

\begin{itemize}
  \item Use phase space signal MC as signal proxy
  \item Use right sideband as bkg proxy. Why?
  \item Dem control plots
  \item Plots of signal MC, sweighted control and control MC
  \item Use resampling for PID variables
  \item correlation matrix
  \item importances?
\end{itemize}

\subsection{Classification of signal and background}

\begin{itemize}
  \item Plot of classifier output for signal MC, sidebands and control signal as well
  \item Plot of classifier output for control sweighted signal and bkg
  \item Give a KS test value for the control channel that shows that the classifier is unbiased
\end{itemize}

\subsection{Optimization of the Figure of Merit}

\begin{itemize}
  \item Choose an appropriate signal window for this (objectively)
  \item We want to optimize the classifier cut according to an objective criterium
  \item Use punzi figure of merit (formula)
  \item Use signal efficiency for s, don't need total number of signal candidates as it can be factorized and doesn't affect the maximum
  \item For background efficiency, take efficiency on sidebands. For total number of bkg candidates, perform initial fit (show plot)
  \item Show plot of FOM depending on cut point
  \item List maximum value, explain that this was used to cut on clf output
\end{itemize}

\section{Determination of the signal efficiency}

\begin{itemize}
  \item Get stripping efficiency from MC
  \item Get trigger efficiency from TISTOS on control channel
\end{itemize}

\chapter{Determination of expected signal branching ratio} % 15

\begin{itemize}
  \item Give an overview of how this was done
\end{itemize}

\section{Maximum Likelihood estimation}

In order to extract an estimate of the number of signal events from the dataset, a \textit{Maximum Likelihood Estimate} (MLE) is performed.
The MLE is a method to determine the parameters $θ$ of a probability distribution $p(x | θ)$, given the data $x$.
It is given by
\begin{equation}
  θ^* = \mathrm{argmax}\ p(θ | x)
  \label{eq:mle}
\end{equation}
where $p(θ | x)$ is the \textit{Likelihood}, which can be obtained by fixing the data $x$ in the probability distribution $p(x | θ)$ and varying the parameters $θ$.

If we are dealing with $N$ identically distributed and uncorrelated events $\vec{x}$, we can express the Likelihood $\mathcal{L}$ as
\begin{equation}
  \mathcal{L}(θ | \vec{x}) = \prod_i^N p(x_i | θ)\:.
\end{equation}

In high energy physics, statistical models often have to discriminate between different categories of events (\eg \textit{signal} or \textit{background}).
This can be realized as a \textit{Mixture Model}
\begin{equation}
  \mathcal{L}(θ, \vec{f} | \vec{x}) = \prod_i^N \sum_j^K f_j p_j(x_i | θ)\:,
\end{equation}
where $K$ is the number of categories and $f_j$ is the mixture weight of the $j$th category with
\begin{equation}
  \sum_j f_j = 1\:.
\end{equation}
The events originating from mixture category $j$ are distributed according to $p_j(x | θ)$.

From this, estimates $f_j^*$ for the mixture weights can be inferred via \eqref{eq:mle}.
But in high energy physics, we are rarely interested in the mixture weights, and more often in the total number of events in a certain category, \eg the number of signal events.
Such an estimate can be obtained through
\begin{equation}
  N_j = f_j N_\text{total}\:.
\end{equation}
To determine the uncertainty of our estimate $N_j$, the uncertainties of $f_j$ (determined from the MLE) and of $N_\text{total}$ have to be propagated, where we have to take into account that $N_\text{total}$ is the result of a Poisson counting experiment.

This manual step can be avoided by modeling the Poissonian fluctuation directly as part of the Likelihood:
\begin{equation}
  \mathcal{L}(θ, \vec{f}, n | \vec{x}, N) = \frac{n^N\mathrm{e}^{-n}}{N!}  \prod_i^N \sum_j^K f_j p_j(x_i | θ)\:,
\end{equation}
where $n$ is the expected value of $N$.

By performing the variable transformation $n f_j \to N_j$ and neglecting the constant factor $\frac{1}{N!}$, we obtain
\begin{equation}
  \mathcal{L}(θ, N_j | \vec{x}, N) = \mathrm{e}^{-\sum_j N_j}  \prod_i^N \sum_j^K N_j p_j(x_i | θ)\:.
\end{equation}
This model, which is referred to as the \textit{Extended Likelihood}\cite{Lyons1986530}, allows a direct estimate of $N_j$, including its uncertainty without manual error propagation.

\begin{itemize}
  \item Mention RooFit
\end{itemize}

%\section{The Profile Likelihood Ratio}
%
%\begin{itemize}
%  \item We want to get a limit, need PLR
%  \item Explain Wilke's theorem
%  \item How do we go about calculating an expected limit? $\rightarrow$ lots of toys
%\end{itemize}
%
%\section{Signal model}
%
%\begin{itemize}
%  \item Double Gaussian
%  \item Constrain this from simulation
%\end{itemize}
%
%\section{Background models}
%
%\begin{itemize}
%  \item Exponential, one resonant in $D$, one not
%\end{itemize}
%
%\section{Validation of fit model}
%
%\begin{itemize}
%  \item Maybe generate toys and look at some pulls
%\end{itemize}
%
%\section{Normalization channel}
%
%\begin{itemize}
%  \item This is $B^0\to J/ψK^{*0}$
%  \item Why do we want a normalization channel? $\rightarrow$ to reduce systematic uncertainties
%  \item Choose similar trigger/stripping setup
%  \item Show fits and give number of expected signal events
%\end{itemize}
%
%\section{Normalization constant}
%
%\begin{itemize}
%  \item Give formula for normalization constant $α$
%  \item Calculate value of $α$
%\end{itemize}
%
%\section{Calculating the expected branching ratio}
%
%\begin{itemize}
%  \item How do we do this?
%  \item Put alpha as constraint into fit
%  \item Make toys, calculate limit for each
%  \item Take median, show plot
%\end{itemize}

\chapter{Systematic uncertainties} % 3

\begin{itemize}
  \item Which systematic uncertainties do we have?
  \item PID resampling
  \item Shape of signal
  \item Trigger efficiency
  \item Dimuon mass resolution
\end{itemize}

\chapter{Conclusion and outlook} % 1

\nocite{pdg}

